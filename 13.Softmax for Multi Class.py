# -*- coding: utf-8 -*-
"""13.Softmax for Multi-Class.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H1TmIPrC2V_QpfrH4eqCSg8wUio_LjSk

# **Softmax for Multi-Class**
"""

# importing libraries
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

# loading iris dataset
iris=load_iris()

# built-in, iris.data will seperate the input features
x=iris.data

# built-in, iris.data will seperate the output features
y=iris.target

# spliting dataset
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)

"""**"multinomial"**<br>
True Softmax regression.<br>
Learns all classes simultaneously with one loss function.<br>
Usually better for multi-class tasks.<br>
Only supported by solvers: lbfgs, newton-cg, saga.<br>

**solver**<br>
This tells which algorithm the model should use to find the best weights (optimization method).<br>
"lbfgs" â†’ default, fast and good for small/medium datasets, supports multinomial (softmax).<br>

"""

model=LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500)

# training the model
model.fit(x_train,y_train)

# predicting for test data
y_pred=model.predict(x_test)

# gathering probabilities
y_prob=model.predict_proba(x_test)

# printing probabilites for 5 five samples
print("Predicted Probabilites: ",y_prob[:5])

# importing accuracy library
from sklearn.metrics import accuracy_score

# priting accuracy by comparing each predicted label with the true label.
print("Accuracy: ", accuracy_score(y_test,y_pred))

