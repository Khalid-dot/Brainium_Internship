# -*- coding: utf-8 -*-
"""9.Cost Functions & Gradient Descent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q_qVffZVwB_9Ycf0eHDF8mCDei615aEO

# **Cost Function & Gradient Descent (Linear Regression)**
"""

# importing numpy library
import numpy as np

"""**Creating a gradient Descent function**"""

def gradient_descent(x,y):
  m_curr=b_curr=0 #initializing m and b as 0 (m: slope of the line, b= intercept)
  iterations=1000
  learning_rate=0.08
  n=len(x) #taking length of x assuming both x and y are same lengh
  for i in range(iterations):
    y_predict=m_curr * x + b_curr #basic formula of linear regression for finding value of y
    cost=(1/n) * sum([val**2 for val in (y-y_predict)]) #formula for finding value of cost function
    md= -(2/n) * sum(x*(y-y_predict)) #formula for finding the derivative of m
    bd= -(2/n) * sum(y-y_predict) #formula for finding derivative of b
    m_curr=m_curr - learning_rate * md #updating m
    b_curr=b_curr - learning_rate * bd #updating b

    print("m: {}, b: {}, cost: {}, iteration: {}".format(m_curr,b_curr,cost,i))

x=np.array([1,2,3,4,5]) #initializing x array via numpy
y=np.array([5,7,9,11,13]) #initializing y array via numpy

# calling function
gradient_descent(x,y)

