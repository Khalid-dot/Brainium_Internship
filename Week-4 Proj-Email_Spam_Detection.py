# -*- coding: utf-8 -*-
"""Week-4 Proj-Email_Spam_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13qYSMR32EIcQ-b42lXUJ3JoMZPn18Izm

# **Week 4 project: Email Spam Detection**
"""

# importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,classification_report, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.svm import SVC

# importing warnings to ignore warnings
import warnings
warnings.filterwarnings('ignore')

"""CSV file has some characters that donâ€™t follow the UTF-8 rules.<br>

UTF-8 and Latin-1 are two different rulebooks.
"""

data=pd.read_csv('/content/email_spam_detection.csv',encoding='latin-1').iloc[:,:2]

data.head()

# renaming columns
data=data.rename(columns={"v1":"Category","v2":"Text"})
data.head()

print("Total Rows: ",len(data))

data.describe()

data.isna().sum()

# printing total count for each class
print("Total Ham: ",data['Category'].value_counts()['ham'])
print("Total Spam: ",data['Category'].value_counts()['spam'])

"""### **Apply Resampling to Deal with the Data Imbalance**"""

# importing utilities
from sklearn.utils import resample

ham=data[data['Category']=='ham']
spam=data[data['Category']=='spam']

# applying resampling to increase spam count equal to ham
resample_spam=resample(spam,
                       replace=True,
                       n_samples=len(ham),
                       random_state=42
)

# concatinating the resambled data with the other class
data=pd.concat([resample_spam,ham])

# printing the count again to check if the imbalance have been removed now
print("Total Ham: ",data['Category'].value_counts()['ham'])
print("Total Spam: ",data['Category'].value_counts()['spam'])

data.head()

# encoding the target class
data['Category']=data['Category'].map({'ham':0,'spam':1})

# checking for null values
print(data['Category'].unique())
print(data['Category'].isna().sum())

data.head()

"""### **Data Training**"""

x_trian,x_test,y_train,y_test=train_test_split(data['Text'],data['Category'],test_size=0.3,random_state=42)

"""It converts text data into numerical features that ML models can understand.<br>

Specifically, it creates a Bag-of-Words representation:<br>

Builds a vocabulary of all unique words in your dataset.<br>

Turns each document/message into a vector of word counts.<br>
"""

from sklearn.feature_extraction.text import CountVectorizer

# stop_words='english' will ignore the common words, max_features will limit the number of words
vectorizer=CountVectorizer(stop_words='english',max_features=3000)

x_train_vec=vectorizer.fit_transform(x_trian) #fit & transform on training data
x_test_vec=vectorizer.transform(x_test) #only transform on test data

# creating Logistic Regression object
model=LogisticRegression(max_iter=1000)

"""### **Logistic Regression**"""

model.fit(x_train_vec,y_train)

y_pred=model.predict(x_test_vec)

svc_model=SVC(kernel='linear')

"""### **Support Vector Machine**"""

svc_model.fit(x_train_vec,y_train)

y_pred_scv=svc_model.predict(x_test_vec)

"""**Printing Results**"""

print("--------------LOGISTIC REGRESSION RESULTS--------------")
print("\nAccuracy: ",accuracy_score(y_test,y_pred))
print("\nClassification Report: ",classification_report(y_test,y_pred))
print("\nConfusion Matrix: ",confusion_matrix(y_test,y_pred))

print("--------------SVM RESULTS--------------")
print("\nAccuracy: ",accuracy_score(y_test,y_pred_scv))
print("\nClassification Report: ",classification_report(y_test,y_pred_scv))
print("\nConfusion Matrix: ",confusion_matrix(y_test,y_pred_scv))

"""### **Testing**"""

input = ["Congratulations, you won free 100 Bitcoins in a lottery"]
new_vec = vectorizer.transform(input)
result=model.predict(new_vec)
if result==0:
  print("Not Spam")
else:
  print("Spam")

