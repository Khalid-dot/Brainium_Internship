# -*- coding: utf-8 -*-
"""Week-5 Proj-Employee_Attrition_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14vi_8fGxKu5o7cRs9uiD0z1cCB0cKc6L

# **Week 5 project: Employee Attrition Predcition**
"""

# importing basic libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# importing scikit learn libraries
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

# loading dataset
emp=pd.read_csv('/content/HR Employee Attrition.csv')
emp.head()

# counting values in the Attrition column
print(emp['Attrition'].value_counts())

no=emp[emp['Attrition']=="No"]
yes=emp[emp['Attrition']=="Yes"]

# resampling data because of unbalance
from sklearn.utils import resample

print(emp['Attrition'].unique())

print("Length Yes, No: ",len(yes),len(no))

"""## **Resampling the unbalanced Data**"""

# performing Resampling
resample_yes = resample(
    yes,
    replace=True,
    n_samples=len(no),
    random_state=42
)

# merging the exiting data with resampled data
emp=pd.concat([no,resample_yes])

print(emp['Attrition'].value_counts())

"""### **Label Encoding**"""

le=LabelEncoder()

# encodign the columns that are not numeric
for col in emp.select_dtypes(include=['object']).columns:
  emp[col]=le.fit_transform(emp[col])

x=emp.drop('Attrition', axis=1)
y=emp['Attrition']

"""### **Standard Scaling**"""

scale=StandardScaler()

sx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)

# scaling the input
x_train=scale.fit_transform(x_train)
x_test=scale.transform(x_test)

"""## **Decision Tree**"""

dt=DecisionTreeClassifier(criterion='gini', max_depth=3,random_state=42)

dt.fit(x_train,y_train)

y_pred=dt.predict(x_test)

print("Accuracy of Decision Tree:",accuracy_score(y_test,y_pred))

print("Classification Report Decision Tree:\n",classification_report(y_test,y_pred))

"""## **Bagging**"""

bag_model=BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=50,
    random_state=42
)

bag_model.fit(x_train,y_train)

y_pred_bag=bag_model.predict(x_test)

print("Classification Report Bagging:\n",classification_report(y_test,y_pred_bag))

"""## **Random Forest**"""

rf=RandomForestClassifier(n_estimators=50,random_state=42)

rf.fit(x_train,y_train)

y_pred_rf=rf.predict(x_test)

print("Classification Report Random Forest:\n",classification_report(y_test,y_pred_rf))

"""## **K Nearest Neighbor**"""

knn=KNeighborsClassifier(n_neighbors=9,metric='euclidean')

knn.fit(x_train,y_train)

y_pred_knn=knn.predict(x_test)

print("Classification Report KNN:\n",classification_report(y_test,y_pred_knn))

"""## **Performing Grid Search**"""

# min samples= This controls the minimum number of samples required to split an internal node.
params_grid={
    "n_estimators": [50,100,150],
    "max_depth": [None,5,10],
    "min_samples_split": [2,5,10]
}

# n_jobs=It controls how many CPU cores to use when running tasks in parallel, -1 means utalize all
grid=GridSearchCV(RandomForestClassifier(random_state=42),params_grid,cv=5,scoring='f1',n_jobs=5)

grid.fit(x_train,y_train)

print("Best Random Forest Parameters:",grid.best_params_)

print("Best F1 Score:",grid.best_score_)