# -*- coding: utf-8 -*-
"""21.XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19USvYOQl57BFbcZ4ZVehnwu_WhqaoMQW

# **XGBoost**
"""

# importing libraries
import numpy as np
from sklearn.datasets import fetch_openml #to fetch the MNIST dataset
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report

# as_frame will fetch the data as numpy array rather than dataFrame
mnist=fetch_openml("mnist_784",version=1,as_frame=False)

# .astype(int) will change the output from string into integer
x,y=mnist["data"],mnist["target"].astype(int)

# normalizing input
x=x/255.0

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

xgb=XGBClassifier(
    objective="multi:softmax", #This tells XGBoost it’s a multi-class classification problem (digits 0–9).
    max_depth=6, #max depth of the tree
    subsample=0.8, #Uses only 80% of training samples per tree (row wise)
    colsample_bytree=0.8, #Uses only 80% of features (columns) per tree.
    eval_metric="mlogloss", #Metric used for evaluation = multiclass log loss (measures error in probability predictions).
    n_estimators=100, #number of trees
    tree_method="gpu_hist", #Fast training algorithm using histograms (good for large datasets like MNIST).
    num_class=10, #number of classes
    learning_rate=0.1 #learning rate
)

"""## **Training**"""

xgb.fit(x_train,y_train)

y_pred=xgb.predict(x_test)

print("XGBoost Accuracy:",accuracy_score(y_test,y_pred))

print("Classification Report:\n",classification_report(y_test,y_pred))

"""## **Testing**"""

import cv2
import matplotlib.pyplot as plt

image=cv2.imread("/content/7.png",cv2.IMREAD_GRAYSCALE)
plt.imshow(image,cmap="gray")
plt.title("Original Image")
plt.axis("off")
plt.show()

# Resize to 28x28
resized=cv2.resize(image,(28,28))

# Invert colors if needed (MNIST digits are white on black background)
if np.mean(resized)>127:
  resized=256-resized

# Flatten & normalize
img_flatten=resized.flatten()/255.0

"""img_flat is a vector of length 784 (28×28).

But XGBoost (like scikit-learn) expects input as a 2D array: (n_samples, n_features).

reshape(1, -1) makes it shape (1, 784) → meaning “1 image, 784 features”.

[0] picks the first (and only) prediction since you gave it one image.
"""

# Make prediction
pred = xgb.predict(img_flatten.reshape(1, -1))[0]

print("Predicted Digit:", pred)

